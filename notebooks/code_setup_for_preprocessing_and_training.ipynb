{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a3c2de",
   "metadata": {},
   "source": [
    "# Creation of needed code for feature selection and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8a509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548de2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "df = pd.read_csv('../data/dataset_initial.csv')\n",
    "df['Weather_Condition_Arr'] = df['Weather_Condition_Arr'].apply(lambda x: literal_eval(x) if str(x)!='nan' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a1db6-a0cd-4f1b-9e61-4dc79f14593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "X, y = df[df.columns.drop('Severity')], df['Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97224030",
   "metadata": {},
   "source": [
    "## Looking at data for additional processing and encoding needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a96b9c-4ee3-40f2-8319-ef5d8e9f87b0",
   "metadata": {},
   "source": [
    "Colums of object type - categorical variables some of them with >2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = df.columns[df.dtypes=='object']\n",
    "objects = df[object_cols]\n",
    "objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64124608-3f8c-44ab-844a-3cc46e2b736a",
   "metadata": {},
   "source": [
    "# Feature selection and encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bedab3-f2c4-4ecc-8616-c6e9bf9a6da0",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a8519-95b5-41fc-88ae-5d5ebd742224",
   "metadata": {},
   "source": [
    "Here feature selection refers to lowering categorical data cardinality and it is performed by sorting categories by frequencies and encoding first 10 as binary features while the others are encoded as separate feature labeled 'Other'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15d3c1",
   "metadata": {},
   "source": [
    "### Weather column feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538188c3",
   "metadata": {},
   "source": [
    "Checking weather condition with multiple conditions present in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Weather_Condition_Arr'].map(lambda arr: len(arr), na_action='ignore')>1]['Weather_Condition_Arr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1baf70",
   "metadata": {},
   "source": [
    "Multiple weather condition will be also one hot encoded with multiple 1s on corresponding values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f785335-6bc8-40b3-a356-810025929254",
   "metadata": {},
   "source": [
    "Example of top 20 weather conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c6a09-b80c-4a6e-b5d3-f73898b91cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "counts = df['Weather_Condition_Arr'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "counts.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab35774-dbbc-422e-b263-61d129ad8999",
   "metadata": {},
   "source": [
    "Creating custom data transformer for weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbffc7-da27-46b7-9c30-eb40559114c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "class WeatherConditionTransformator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X):\n",
    "        counts = df['Weather_Condition_Arr'].explode().value_counts()\n",
    "        self.top_weathers = counts[:10]\n",
    "        return self\n",
    "    \n",
    "    def _weather_condition_mapper(self,weather_condition_arr):\n",
    "        weathers = set()\n",
    "        for w in weather_condition_arr:\n",
    "            if w in self.top_weathers:\n",
    "                w_to_add = w\n",
    "            else:\n",
    "                w_to_add='Weather_Other'\n",
    "            weathers.add(w_to_add)\n",
    "        return list(weathers)\n",
    "            \n",
    "    def transform(self, X, y=None):\n",
    "        X['Weather_Condition_Arr'] = X['Weather_Condition_Arr'].map(self._weather_condition_mapper,na_action='ignore')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1019c-92ce-401f-b2f8-7ef439aa3809",
   "metadata": {},
   "source": [
    "### States feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e1eb6-f86e-481a-99d2-10a3fa6b6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df['State'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb9556-8744-4d6d-937c-794445dae74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateTranformator(BaseEstimator,TransformerMixin):\n",
    "    def fit(self,X):\n",
    "        self.top_states = X['State'].value_counts()[:10].index\n",
    "        return self\n",
    "        \n",
    "    def _map_state(self,state):\n",
    "        if state in self.top_states:\n",
    "            return state\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X['State']=X['State'].map(self._map_state,na_action='ignore')\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f625e5c-c275-4e61-8939-492fd6f33d9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Final feature selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749bb823-f101-4518-a25a-ba9b742a0a51",
   "metadata": {},
   "source": [
    "Feature selection composes of weather feature selection and state feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea58454-5af8-4f27-b2b9-2395a36050ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.wct = WeatherConditionTransformator()\n",
    "        self.st = StateTranformator()\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.wct.fit(X)\n",
    "        self.st.fit(X)\n",
    "        return self\n",
    "        \n",
    "    def transform(self,X):\n",
    "        X = self.wct.transform(X)\n",
    "        X = self.st.transform(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580aa1c-7422-40eb-81ec-7febe6b3ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtale\n",
    "\n",
    "X_sample = X.sample(100000).copy()\n",
    "\n",
    "p = FeatureSelector()\n",
    "\n",
    "X_sample = p.fit_transform(X_sample)\n",
    "dtale.show(X_sample[1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c09fc3-735b-49f7-a9e4-3f2a8481ad36",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccfb0e-52d0-4f07-b575-426bcc0ecb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(TransformerMixin):\n",
    "    def __init__(self,categorical_columns):\n",
    "        self.categorical_columns=categorical_columns\n",
    "    \n",
    "    def fit(self,X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        for column in self.categorical_columns:\n",
    "            tempdf = pd.get_dummies(X[column], prefix=column,drop_first=True)\n",
    "            X = pd.merge(\n",
    "                left=X,\n",
    "                right=tempdf,\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "            )\n",
    "            X = X.drop(columns=column)\n",
    "        tempdf = pd.get_dummies(X['Weather_Condition_Arr'].explode()).groupby(level=0).sum()\n",
    "        X = pd.merge(\n",
    "            left=X,\n",
    "            right=tempdf,\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\n",
    "        X = X.drop(columns=\"Weather_Condition_Arr\")\n",
    "        return X        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd71b3-1453-47ff-aa98-8231b1335ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = ['Side', 'State', 'Amenity','Bump','Crossing',\n",
    "                         'Give_Way', 'Junction','No_Exit',\n",
    "                         'Railway','Roundabout','Station','Stop',\n",
    "                         'Traffic_Calming','Traffic_Signal','Turning_Loop',\n",
    "                         'Sunrise_Sunset','Civil_Twilight','Nautical_Twilight',\n",
    "                         'Astronomical_Twilight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8567bfc-fe54-4a3b-a1f8-ff79f71120dd",
   "metadata": {},
   "source": [
    "## Final feature selection and coding example usage and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac21813-5da9-4ba1-a6a1-1635e53dd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample = df.sample(100000).copy()\n",
    "fs = FeatureSelector()\n",
    "X_train_sample = fs.fit_transform(X_train_sample)\n",
    "enc = Encoder(categorical_variables)\n",
    "dtale.show(enc.fit_transform(X_train_sample)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f0bc4-48d8-453f-8a43-406eb1e2554f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating code infrastracture for kfold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280659c7-6eda-49be-9b56-a3b2f6190cc0",
   "metadata": {},
   "source": [
    "Using custom code is necessary to select features and encode each k-1 training fold and to ensure no spillage. As that would be the case of feature selection and encoding on whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8b939-63da-405e-b755-5522bb1f3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "class BaseTrainer():\n",
    "    def __init__(self,X_train_val,Y_train_val,cat_vars=[],reg_vars=[],vtype=\"k-fold\", k=3,split=0.8):\n",
    "        self.vtype=vtype\n",
    "        self.cat_vars=cat_vars\n",
    "        if(vtype==\"k-fold\"):\n",
    "            self._init_kfold(X_train_val,Y_train_val,cat_vars,reg_vars,k)\n",
    "            return\n",
    "        elif(vtype==\"hold-out\"):\n",
    "            self._init_hold_out(X_train_val,Y_train_val,cat_vars,reg_vars,split)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid type {type} supported types: kfold and hold-out\")\n",
    "            \n",
    "    def _init_kfold(self,X_train_val,Y_train_val,cat_vars,reg_vars,k):\n",
    "        self.X_train_val=X_train_val\n",
    "        self.Y_train_val=Y_train_val\n",
    "        self.kf = StratifiedKFold(n_splits=k)\n",
    "        \n",
    "    def _init_hold_out(self,X_train_val,Y_train_val,cat_vars,reg_vars,split):\n",
    "        X_train,X_val,Y_train,Y_val = train_test_split(X_train_val,Y_train_val,train_size=split,stratify=Y_train_val)\n",
    "        self.X_train,self.X_valid = self._select_and_encode(X_train, X_val)\n",
    "        self.Y_train,self.Y_valid = self._encode_target(Y_train,Y_val)\n",
    "        \n",
    "        \n",
    "    def _split_using_index(self,train_index,valid_index):\n",
    "        X_train = self.X_train_val.iloc[train_index].copy()\n",
    "        X_valid = self.X_train_val.iloc[valid_index].copy()\n",
    "        Y_train = self.Y_train_val.iloc[train_index].copy()\n",
    "        Y_valid = self.Y_train_val.iloc[valid_index].copy()\n",
    "        return X_train, X_valid, Y_train, Y_valid\n",
    "\n",
    "    def train_and_validate(self,params):\n",
    "        if(self.vtype==\"k-fold\"):\n",
    "            valid_score=self._train_and_validate_kfold(params)\n",
    "        if(self.vtype==\"hold-out\"):\n",
    "            valid_score=self._train_and_validate_hold_out(params)\n",
    "        return valid_score\n",
    "            \n",
    "    def _train_and_validate_kfold(self,params):\n",
    "        valid_scores = []\n",
    "        for train_index, valid_index in self.kf.split(self.X_train_val,self.Y_train_val):\n",
    "            X_train, X_valid, Y_train, Y_valid = self._split_using_index(train_index,valid_index)\n",
    "            Y_train, Y_valid = self._encode_target(Y_train,Y_valid)\n",
    "            X_train, X_valid = self._select_and_encode(X_train,X_valid)\n",
    "            trained_model = self._train_model(params,X_train,Y_train,X_valid,Y_valid)\n",
    "            valid_score = self._validate_model(trained_model,X_valid,Y_valid)\n",
    "            valid_scores.append(valid_score)\n",
    "        return np.mean(valid_scores)\n",
    "    \n",
    "    def _train_and_validate_hold_out(self,params):\n",
    "        trained_model = self._train_model(params,self.X_train,self.Y_train,self.X_valid,self.Y_valid)\n",
    "        valid_score = self._validate_model(trained_model,self.X_valid,self.Y_valid)\n",
    "        return valid_score\n",
    "                \n",
    "    def _train_model(self,params,X_train,y_train,X_val=None,Y_val=None):\n",
    "        raise NotImplementedException\n",
    "    \n",
    "    def _validate_model(self,model):\n",
    "        raise NotImplementedException\n",
    "        \n",
    "    def _select_and_encode(self,X_train, X_valid):\n",
    "        fs = FeatureSelector()\n",
    "        X_train = fs.fit_transform(X_train)\n",
    "        X_valid = fs.transform(X_valid)\n",
    "        encoder = Encoder(self.cat_vars)\n",
    "        X_train = encoder.fit_transform(X_train)\n",
    "        X_valid = encoder.fit_transform(X_valid)\n",
    "        return X_train,X_valid\n",
    "    \n",
    "    def _encode_target(self,Y_train, Y_valid):\n",
    "        return Y_train-1, Y_valid-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d72640-6493-4425-b170-e5f664bd351a",
   "metadata": {},
   "source": [
    "# Model specific steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc6265-0256-4603-9c86-d0a82b9d85e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd15d1a-fd6a-4f4f-958d-51ac375b68b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e70131-8671-411b-8128-0b5844001ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "from fast_tabnet.core import *\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class TabNetTrainer(BaseTrainer):\n",
    "    def __init__(self,X_train_val,Y_train_val,cat_vars=[],reg_vars=[],vtype=\"k-fold\", k=3,split=0.8,epochs=50):\n",
    "        self.epochs=epochs\n",
    "        super().__init__(X_train_val,Y_train_val,cat_vars,reg_vars,vtype, k,split)\n",
    "\n",
    "\n",
    "    def _train_model(self,params,X_train,y_train,X_val,Y_val):\n",
    "        bs= params.pop('batch_size')\n",
    "        lr= params.pop('lr')\n",
    "        to = self._fastaify_data(X_train,y_train,X_val,Y_val)\n",
    "        dls = to.dataloaders(bs)\n",
    "        \n",
    "        optimizer = params.pop('optimizer')\n",
    "    \n",
    "        model = TabNetModel(get_emb_sz(to), len(to.cont_names), dls.c,**params)\n",
    "        class_weights = self._get_weights(y_train)\n",
    "        learn = Learner(dls, model,CrossEntropyLossFlat(weight=class_weights), opt_func=optimizer, lr=lr, metrics=[MatthewsCorrCoef()])\n",
    "        learn.fit_one_cycle(self.epochs)\n",
    "        return learn\n",
    "    \n",
    "    def _validate_model(self,model,X_val=None,Y_val=None):\n",
    "        return float(model.validate()[1])\n",
    "    \n",
    "    def _get_weights(self,Y_train):\n",
    "        class_weights=compute_class_weight('balanced',classes=[0,1,2,3],y=Y_train)\n",
    "        class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "        return class_weights\n",
    "        \n",
    "    def _fastaify_data(self,X_train,Y_train,X_val,Y_val):\n",
    "        train = pd.merge(\n",
    "            left=X_train,\n",
    "            right=Y_train,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "\n",
    "        val = pd.merge(\n",
    "            left=X_val,\n",
    "            right=Y_val,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "\n",
    "        train_len = len(train)\n",
    "        val_len = len(val)\n",
    "        splits = [list(range(0,train_len)),list(range(train_len,train_len+val_len))]\n",
    "\n",
    "        train_val = pd.concat([train,val])\n",
    "        train_val.reset_index(drop=True)\n",
    "        \n",
    "        cont_names = ['Start_Lat','Start_Lng','End_Lat','End_Lng','Distance(mi)',\n",
    "            'Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)',\n",
    "            'Visibility(mi)','Wind_Speed(mph)','Precipitation(in)','Wind_SN',\n",
    "            'Wind_EW']\n",
    "\n",
    "        cat_names = [col for col in train_val.columns]\n",
    "        _=[cat_names.remove(cont_name) for cont_name in cont_names+['Severity']]\n",
    "        \n",
    "        to = TabularPandas(\n",
    "            train_val, \n",
    "            [Categorify,FillMissing], \n",
    "            cat_names, cont_names, \n",
    "            y_names='Severity', \n",
    "            y_block = CategoryBlock(), \n",
    "            splits=splits\n",
    "        )\n",
    "        \n",
    "        return to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d0411-2f26-46b1-936c-cd67929369e1",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373943b-fecc-4e1f-a655-5084f2f8102e",
   "metadata": {},
   "source": [
    "Selection of data percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd2b2a-9182-4dab-ba85-57a38b5304bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_test, Y_train_val_test = X.copy(),y.copy()\n",
    "X_train_val,X_val,Y_train_val,Y_val = train_test_split(X_train_val_test, Y_train_val_test,train_size=0.1,stratify=Y_train_val_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f5953-fd1c-429c-858b-833da476fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK,hp,tpe,Trials,fmin\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "tabnet_large_space={\n",
    "    \"lookahead\": hp.choice(\"lookahead\",[False,True]),\n",
    "    \"optimizer\": hp.choice('optimizer',[\n",
    "        {\n",
    "            \"opttype\":\"Adam\",\n",
    "             \"wd\":hp.loguniform('wdadam', np.log(0.0001), np.log(0.3))\n",
    "        },\n",
    "        {\n",
    "            \"opttype\":\"SGD\",\n",
    "            \"wd\":hp.loguniform('wdsgd', np.log(0.0001), np.log(0.3))\n",
    "        },\n",
    "        {\n",
    "            \"opttype\":\"RAdam\",\n",
    "            \"wd\":hp.loguniform('wdradam', np.log(0.0001), np.log(0.3))\n",
    "        }\n",
    "    ]),\n",
    "    \"n\":scope.int(hp.choice(\"n\",[8,64,128])),\n",
    "    \"n_steps\":scope.int(hp.quniform(\"n_steps\",3,10,1)),\n",
    "    \"gamma\":hp.uniform(\"gamma\",1,2),\n",
    "    \"momentum\":hp.uniform(\"momentum\",0,1),\n",
    "    \"lr\":hp.choice(\"lr\",[0.005,0.01,0.02,0.025]),\n",
    "    \"batch_size\":hp.quniform(\"batch_size\",12,17,1),\n",
    "    \"virtual_batch_size\":hp.quniform(\"virtual_batch_size\",8,11,1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15db602-e36c-4723-a3d7-af297c4af194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(opttype,opt_params,lookahead):\n",
    "    OPT_DICT = {\n",
    "        \"Adam\":Adam,\n",
    "        \"RAdam\":RAdam,\n",
    "        \"SGD\":SGD\n",
    "    }\n",
    "    opt_constructor = OPT_DICT[opttype]\n",
    "    if lookahead:\n",
    "        partial_opt = lambda spliter,lr: Lookahead(opt_constructor(spliter,lr,**opt_params))\n",
    "        optimizer = partial_opt\n",
    "    else:\n",
    "        optimizer = partial(opt_constructor,**opt_params)\n",
    "    return optimizer\n",
    "\n",
    "def process_params(params):\n",
    "    params['batch_size'] = int(np.power(2,params['batch_size']))\n",
    "    params['virtual_batch_size'] = int(np.power(2,params['virtual_batch_size']))\n",
    "\n",
    "    opt_params = params.pop('optimizer')\n",
    "    opttype = opt_params.pop('opttype')\n",
    "    lookahead = params.pop('lookahead')\n",
    "    optimizer = get_optimizer(opttype,opt_params,lookahead)\n",
    "    \n",
    "    params['optimizer'] = optimizer\n",
    "    n=params.pop('n')\n",
    "    params['n_d']=n\n",
    "    params['n_a']=n\n",
    "    return params\n",
    "    \n",
    "\n",
    "def tabnet_fn(params):\n",
    "    params = process_params(params)\n",
    "    print(params)\n",
    "    tabnet_trainer = TabNetTrainer(X_train_val,Y_train_val,vtype=\"hold-out\",split=0.8,epochs=1)\n",
    "    return -tabnet_trainer.train_and_validate(params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468d3c5-2464-4d3d-9d43-f54b3bff2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defaults.use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be6c4bd-8f4f-428e-b6e4-7c7468899961",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_hyperparams = fmin(fn = tabnet_fn,\n",
    "                        space = tabnet_large_space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 10,\n",
    "                        trials = trials)\n",
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c056d4-cadc-4c84-8984-beca52b707af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46adef15-2e22-4695-882e-c0d4c8e6eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import class_weight\n",
    "import xgboost\n",
    "\n",
    "class XGBoostTrainer(BaseTrainer):\n",
    "    def _train_model(self,params,X_train,y_train,X_val,Y_val):\n",
    "        num_round = params.pop('num_round')\n",
    "        model = xgboost.XGBClassifier(**params,verbosity=2)\n",
    "        \n",
    "        classes_weights = class_weight.compute_sample_weight(\n",
    "            class_weight='balanced',\n",
    "            y=y_train\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, num_round=num_round, sample_weight=classes_weights)\n",
    "        return model\n",
    "    \n",
    "    def _validate_model(self,model,X_val=None,Y_val=None):\n",
    "        return matthews_corrcoef(Y_val,model.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960c271-88f6-465f-84d8-37ccfeda58db",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83d08e-50d1-42ef-8086-b22ef6a3ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_test, Y_train_val_test = X.copy(),y.copy()\n",
    "X_train_val,X_val,Y_train_val,Y_val = train_test_split(X_train_val_test, Y_train_val_test,train_size=0.7,stratify=Y_train_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8909e56-6011-41ed-9a73-d2fd567f7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK,hp,tpe,Trials,fmin\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "\n",
    "xgboost_large_space = {\n",
    "    \"eta\":hp.uniform(\"eta\",0.01,0.3),\n",
    "    \"gamma\":hp.uniform(\"gamma\",0,10),\n",
    "    \"max_depth\":scope.int(hp.quniform(\"max_depth\",3,10,1)),\n",
    "    \"min_child_weight\":hp.uniform(\"min_child_weight\",0,10),\n",
    "    \"max_delta_step\":hp.uniform(\"max_delta_step\",1,10),\n",
    "    \"subsample\":hp.uniform(\"subsample\",0.3,1),\n",
    "    \"lambda\":hp.uniform(\"lambda\",0,5),\n",
    "    \"alpha\":hp.uniform(\"alpha\",0,5),\n",
    "    \"num_round\":scope.int(hp.quniform(\"num_round\",50,200))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b5b0a-51c1-426a-b5de-3eb9c1548420",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = ['Side', 'State', 'Amenity','Bump','Crossing',\n",
    "                         'Give_Way', 'Junction','No_Exit',\n",
    "                         'Railway','Roundabout','Station','Stop',\n",
    "                         'Traffic_Calming','Traffic_Signal','Turning_Loop',\n",
    "                         'Sunrise_Sunset','Civil_Twilight','Nautical_Twilight',\n",
    "                         'Astronomical_Twilight']\n",
    "\n",
    "def xgboost_fn(params):\n",
    "    xgboost_trainer = XGBoostTrainer(X_train_val,Y_train_val,cat_vars=categorical_variables,vtype=\"hold-out\",split=0.8)\n",
    "    return -xgboost_trainer.train_and_validate(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2af96-7cf8-4392-8234-df9b779dd809",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_hyperparams = fmin(fn = xgboost_fn,\n",
    "                        space = xgboost_large_space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 10,\n",
    "                        trials = trials)\n",
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d328f3-05cd-4abc-91d6-0755b9261820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(Y_val,clf.predict(X_val))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb966b6-10e7-4a9c-92b2-775a223de952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "plot_importance(clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
